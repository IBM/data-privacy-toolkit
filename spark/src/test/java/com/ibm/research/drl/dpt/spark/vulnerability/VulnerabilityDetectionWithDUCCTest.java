package com.ibm.research.drl.dpt.spark.vulnerability;

import com.ibm.research.drl.dpt.configuration.DataTypeFormat;
import com.ibm.research.drl.dpt.datasets.CSVDatasetOptions;
import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.*;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Disabled;
import org.junit.jupiter.api.Test;

import java.util.*;

import static org.apache.spark.sql.functions.*;
import static org.hamcrest.CoreMatchers.is;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;

public class VulnerabilityDetectionWithDUCCTest {
    private SparkSession spark;

    @BeforeEach
    public void setUp() {
        SparkConf sparkConf =
                (new SparkConf())
                        .setMaster("local[1]")
                        .setAppName("test")
                        .set("spark.ui.enabled", "false")
                        .set("spark.app.id", UUID.randomUUID().toString())
                        .set("spark.driver.host", "localhost")
                        .set("spark.sql.shuffle.partitions", "1");

        spark = SparkSession.builder().sparkContext(new SparkContext(sparkConf)).getOrCreate();
    }

    @AfterEach
    public void tearDown() {
        if (Objects.nonNull(spark)) {
            spark.stop();
        }
    }

    @Test
    public void testCorrectness() {
        VulnerabilityDetectionWithDUCC vulnerabilityDetectionWithDucc = new VulnerabilityDetectionWithDUCC();

        final Dataset<Row> dataset = SparkUtils.createDataset(
                spark,
                VulnerabilityDetectionWithDUCCTest.class.getResource("/adult-10-30000.data.csv").getFile(),
                DataTypeFormat.CSV,
                new CSVDatasetOptions(false, ',', '"', false)
        );

        Collection<List<String>> vulnerabilities = vulnerabilityDetectionWithDucc.findVulnerabilities(dataset, 5, 0);

        assertNotNull(vulnerabilities);
        assertFalse(vulnerabilities.isEmpty());

        VulnerabilityDetectionWithBruteForce vulnerabilityDetectionWithBruteForce = new VulnerabilityDetectionWithBruteForce();

        Collection<List<String>> knownCorrect = vulnerabilityDetectionWithBruteForce.findVulnerabilities(dataset, 5, 0);

        assertThat(vulnerabilities.size(), is(knownCorrect.size()));
    }

    @Test
    public void testCorrectnessReport() {
        VulnerabilityDetectionWithDUCC vulnerabilityDetectionWithDucc = new VulnerabilityDetectionWithDUCC();

        final Dataset<Row> dataset = SparkUtils.createDataset(
                spark,
                VulnerabilityDetectionWithDUCCTest.class.getResource("/adult-10-30000.data.csv").getFile(),
                DataTypeFormat.CSV,
                new CSVDatasetOptions(false, ',', '"', false)
        );

        final int k = 2;
        Collection<List<String>> vulnerabilities = vulnerabilityDetectionWithDucc.findVulnerabilities(dataset, k, 0);

        assertNotNull(vulnerabilities);
        assertFalse(vulnerabilities.isEmpty());

        Map<List<String>, Long> v = vulnerabilityDetectionWithDucc.computeOffendingRecords(dataset, vulnerabilities, k);

        assertNotNull(v);
        assertFalse(v.isEmpty());
        assertThat(v.size(), is(vulnerabilities.size()));
    }
}