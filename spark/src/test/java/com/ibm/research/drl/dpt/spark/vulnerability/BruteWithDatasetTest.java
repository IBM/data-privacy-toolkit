/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2018                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.vulnerability;

import com.ibm.research.drl.dpt.configuration.DataTypeFormat;
import com.ibm.research.drl.dpt.datasets.CSVDatasetOptions;
import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import com.ibm.research.drl.dpt.vulnerability.IPVVulnerability;
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import java.io.IOException;
import java.util.Collection;
import java.util.Collections;
import java.util.Objects;
import java.util.UUID;

import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;

public class BruteWithDatasetTest {

    private SparkSession spark;

    @BeforeEach
    public void setUp() {
        SparkConf sparkConf =
                (new SparkConf())
                        .setMaster("local[1]")
                        .setAppName("test")
                        .set("spark.ui.enabled", "false")
                        .set("spark.app.id", UUID.randomUUID().toString())
                        .set("spark.driver.host", "localhost")
                        .set("spark.sql.shuffle.partitions", "1");

        spark = SparkSession.builder().sparkContext(new SparkContext(sparkConf)).getOrCreate();
    }

    @AfterEach
    public void tearDown() {
        if (Objects.nonNull(spark)) {
            spark.stop();
        }
    }

    @Test
    public void testStandard() throws IOException {
        BruteWithDataset brute = new BruteWithDataset();

        final Dataset<Row> dataset = SparkUtils.createDataset(
                spark,
                this.getClass().getResource("/adult-10-30000.data.csv").getFile(),
                DataTypeFormat.CSV,
                new CSVDatasetOptions(false, ',', '"', false)
        );

        Collection<IPVVulnerability> vulnerabilities = brute.findVulnerabilities(dataset,
                Collections.emptySet(),
                5,
                0
        );

        assertNotNull(vulnerabilities);
        assertFalse(vulnerabilities.isEmpty());
    }
}