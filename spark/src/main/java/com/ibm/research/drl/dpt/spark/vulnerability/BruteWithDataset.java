/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2018                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.vulnerability;

import com.ibm.research.drl.dpt.configuration.DataTypeFormat;
import com.ibm.research.drl.dpt.datasets.CSVDatasetOptions;
import com.ibm.research.drl.dpt.datasets.DatasetOptions;
import com.ibm.research.drl.dpt.generators.ItemSet;
import com.ibm.research.drl.dpt.generators.LayerGenerator;
import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import com.ibm.research.drl.dpt.vulnerability.IPVVulnerability;
import org.apache.commons.cli.*;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Objects;
import java.util.Set;
import java.util.stream.Collectors;

import static org.apache.spark.sql.functions.*;

public class BruteWithDataset {
    private static final String AGGREGATED_COUNT_NAME = "__AGGREGATED_COUNT_NAME__";
    private static final Logger logger = LogManager.getLogger(BruteWithDataset.class);

    public static void main(String[] args) {

        Options options = new Options();

        Option dataOption = new Option("d", "data", true, "Path to the dataset to be analyzes (required)");
        dataOption.setRequired(true);

        Option kOption = new Option("k", "kvalue", true, "k-private requirement");
        kOption.setRequired(true);

        options.addOption(dataOption);
        options.addOption(kOption);
        options.addOption(new Option("f", "fieldSeparator", true, "Field separator (default: ',' )"));
        options.addOption(new Option("q", "quoteCharacter", true, "Quote character (default: '\"'"));
        options.addOption(new Option("h", "header", false, "Enable treating of the first row as header"));
        options.addOption(new Option("t", "type", true, "File type " + Arrays.toString(new DataTypeFormat[]{DataTypeFormat.CSV, DataTypeFormat.PARQUET}) + " (default: CSV)"));
        options.addOption(new Option("e", "exclude", true, "Comma separated list of fields than should to be excluded"));
        Option o = new Option("o", "output", true, "Output directory/file");
        o.setRequired(true);
        options.addOption(o);
        options.addOption(new Option("M", "maxSize", true, "Maximum size for exploration"));

        CommandLineParser parser = new DefaultParser();

        try {
            CommandLine cmd = parser.parse(options, args);

            final int k = Integer.parseInt(cmd.getOptionValue("k"));

            final String delimiter = cmd.getOptionValue("f", ",");
            final String quote = cmd.getOptionValue("q","\"");

            final DataTypeFormat format = cmd.hasOption("t") ? DataTypeFormat.valueOf(cmd.getOptionValue("t")) : DataTypeFormat.CSV;

            final DatasetOptions datasetOptions = new CSVDatasetOptions(cmd.hasOption("h"), delimiter.charAt(0), quote.charAt(0), false);

            SparkContext sc = SparkUtils.createSparkContext("BruteExplorer");
            SparkSession spark = SparkSession.builder().sparkContext(sc).getOrCreate();

            final Set<Integer> excludedFields = buildExcludedFieldsList(cmd.getOptionValue("e"));

            BruteWithDataset bruteWithDataset = new BruteWithDataset();
            final Dataset<Row> dataset = SparkUtils.createDataset(spark, cmd.getOptionValue("d"), format, datasetOptions);

            int maxSize = Integer.parseInt(cmd.getOptionValue("M", "0"));

            logger.info("Testing " + cmd.getOptionValue("d") + " for k = " + k + " with max size " + maxSize );

            Collection<IPVVulnerability> vulnerabilities = bruteWithDataset.findVulnerabilities(dataset, excludedFields, k, maxSize);

            try(JavaSparkContext jsc = new JavaSparkContext(sc);) {
                jsc.parallelize(new ArrayList<>(vulnerabilities)).map(IPVVulnerability::toString).saveAsTextFile(cmd.getOptionValue("o"));
            }
            spark.stop();
        } catch (ParseException e) {
            System.out.println(e.getMessage());

            HelpFormatter helpFormatter = new HelpFormatter();
            String header = "Vulnerability detection\n\n";
            String footer = "\n";
            helpFormatter.printHelp("VulnerabilityDetection", header, options, footer, true);
            System.exit(1);
        }
    }

    public static Set<Integer> buildExcludedFieldsList(String excludedFieldsString) {
        if (Objects.isNull(excludedFieldsString) || excludedFieldsString.isEmpty()) {
            return Collections.emptySet();
        }

        return Arrays.stream(excludedFieldsString.split(",")).
                map(Integer::parseInt).
                collect(Collectors.toSet());
    }

    public static Collection<IPVVulnerability> findVulnerabilities(Dataset<Row> dataset, Set<Integer> excludedFields, int k, int maxSize) {
        final String[] columnNames = dataset.columns();
        final LayerGenerator generator = new LayerGenerator(columnNames.length);

        while (generator.hasNext()) {
            final ItemSet itemSet = generator.next();

            if (hasReachedMaxSize(maxSize, itemSet)) {
                logger.info("Max size reached");
                break;
            }

            logger.info("Testing" + itemSet);

            if (containsExcludedField(itemSet, excludedFields)) {
                logger.info("Skipping " + itemSet + " because contains excluded fields");
                continue;
            }

            final Column[] selectionColumns = computeSelectColumn(itemSet, columnNames);
            
            if (isQuasiIdentifier(dataset, selectionColumns, k)) {
                logger.info("Banning " + itemSet);
                generator.ban(itemSet);
            }
        }

        return generator.getBanned().parallelStream().
                map( itemSet -> new IPVVulnerability(itemSet, k)).
                collect(Collectors.toList());
    }

    private static boolean hasReachedMaxSize(int maxSize, ItemSet itemSet) {
        return maxSize > 0 &&
                itemSet.size() > maxSize;
    }

    private static boolean isQuasiIdentifier(Dataset<Row> dataset, Column[] selectionColumns, int k) {
        Dataset<Row> selection = dataset.
                groupBy(selectionColumns).
                agg(count(lit(1)).as(AGGREGATED_COUNT_NAME)).
                filter(col(AGGREGATED_COUNT_NAME).lt(lit(k)));

        return selection.count() > 0;
    }

    private static Column[] computeSelectColumn(ItemSet itemSet, String[] columnNames) {
        final Column[] selectColumns = new Column[itemSet.size()];

        int i = 0;
        for (int item : itemSet.getItems()) {
            selectColumns[i++] = col(columnNames[item]);
        }

        return selectColumns;
    }

    private static boolean containsExcludedField(ItemSet itemSet, Set<Integer> excludedFields) {
        for (final int item : itemSet.getItems()) {
            if (excludedFields.contains(item)) return true;
        }
        
        return false;
    }
}
