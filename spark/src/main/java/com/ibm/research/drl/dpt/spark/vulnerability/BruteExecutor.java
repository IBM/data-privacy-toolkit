/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2016                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.vulnerability;

import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import com.ibm.research.drl.dpt.vulnerability.IPVVulnerability;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaRDD;

import java.io.IOException;
import java.util.Collection;


public class BruteExecutor {

    /**
     * The entry point of application.
     *
     * @param args the input arguments
     * @throws IOException the io exception
     */
    public static void main(String[] args) throws IOException {
        Options options = new Options();

        Option dataOption = new Option("d", "data", true, "Path to the dataset to be analyzes (required)");
        dataOption.setRequired(true);

        Option kOption = new Option("k", "kvalue", true, "k-private requirement");
        kOption.setRequired(true);

        options.addOption(dataOption);
        options.addOption(kOption);
        options.addOption(new Option("f", "fieldSeparator", true, "Field separator"));
        options.addOption(new Option("h", "header", false, "Enable treating of the first row as header"));
        options.addOption(new Option("b", "batch", true, "Batch size"));

        CommandLineParser parser = new DefaultParser();

        CommandLine cmd;

        try {
            cmd = parser.parse(options, args);
        } catch (ParseException e) {
            throw new RuntimeException("invalid arguments");
        }


        SparkContext sc = SparkUtils.createSparkContext("Brute");
        JavaRDD<String> inputRDD = SparkUtils.createTextFileRDD(sc, cmd.getOptionValue("d"));

        int k = Integer.parseInt(cmd.getOptionValue("k"));

        String delimiter = ",";
        if (cmd.hasOption("f")) {
            delimiter = cmd.getOptionValue("f");
        }

        int batchSize = 1;
        if (cmd.hasOption("b")) {
            batchSize = Integer.parseInt(cmd.getOptionValue("b"));
        }

        Collection<IPVVulnerability> vulnerabilities = BruteSpark.run(inputRDD, k, batchSize, cmd.hasOption("h"), delimiter, '"');

        for(IPVVulnerability vulnerability: vulnerabilities) {
            System.out.println(vulnerability);
        }

        sc.stop();
    }

}
