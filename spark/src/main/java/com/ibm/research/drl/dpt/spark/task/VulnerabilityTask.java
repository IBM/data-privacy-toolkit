/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2023                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.task;

import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.ibm.research.drl.dpt.IPVAlgorithm;
import com.ibm.research.drl.dpt.exceptions.MisconfigurationException;
import com.ibm.research.drl.dpt.processors.FormatProcessor;
import com.ibm.research.drl.dpt.processors.FormatProcessorFactory;
import com.ibm.research.drl.dpt.schema.IdentifiedType;
import com.ibm.research.drl.dpt.spark.dataset.reference.DatasetReference;
import com.ibm.research.drl.dpt.spark.task.option.VulnerabilityTaskOptions;
import com.ibm.research.drl.dpt.spark.vulnerability.BruteWithDataset;
import com.ibm.research.drl.dpt.util.JsonUtils;
import com.ibm.research.drl.dpt.vulnerability.IPVVulnerability;
import com.ibm.research.drl.dpt.vulnerability.brute.ParallelBrute;
import com.ibm.research.drl.dpt.vulnerability.ducc.DUCC;
import com.ibm.research.drl.dpt.vulnerability.fpvi.FPVI;
import com.ibm.research.drl.dpt.vulnerability.mtra.MTRA;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;


import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.*;
import java.util.stream.Collectors;

public class VulnerabilityTask extends SparkTaskToExecute {
    private final VulnerabilityTaskOptions taskOptions;
    private final ObjectMapper mapper;

    @JsonCreator
    public VulnerabilityTask(
            @JsonProperty("task") String task,
            @JsonProperty("inputOptions") DatasetReference inputOptions,
            @JsonProperty("outputOptions") DatasetReference outputOptions,
            @JsonProperty("taskOptions") VulnerabilityTaskOptions taskOptions
    ) {
        super(task, inputOptions, outputOptions);

        this.taskOptions = taskOptions;
        mapper = JsonUtils.MAPPER;
    }

    public void processFile(InputStream input, OutputStream output) throws IOException, MisconfigurationException {
        final IPVAlgorithm ipvAlgorithm = generateAlgorithm();

        FormatProcessor formatProcessor = FormatProcessorFactory.getProcessor(this.getInputFormat());

        Map<IPVVulnerability, List<Integer>> vulnerabilities = formatProcessor.identifyVulnerabilitiesStream(
                input,
                ipvAlgorithm,
                this.getInputFormat(),
                this.getInputOptions(),
                this.getTaskOptions().isFullReport(),
                this.getTaskOptions().getK()
        );

        writerReport(output, vulnerabilities);
    }

    private IPVAlgorithm generateAlgorithm() {

        switch (getTaskOptions().getAlgorithm()) {
            case FPVI:
                return new FPVI(getTaskOptions().getnThreads(), getTaskOptions().getBatchSize(), 1.5, getTaskOptions().getK());

            case DUCC:
                return new DUCC(getTaskOptions().getnThreads(), getTaskOptions().getStrategy(), getTaskOptions().getK());

            case MTRA:
                return new MTRA(getTaskOptions().getnThreads(), 1.5);

            case BRUTE:
            default:
                return new ParallelBrute(getTaskOptions().getK(), getTaskOptions().getnThreads());
        }
    }

    private VulnerabilityTaskOptions getTaskOptions() {
        return this.taskOptions;
    }


    private List<String> getQuasiIdentifiers(Map<IPVVulnerability, List<Integer>> vulnerabilities) {
        return vulnerabilities.keySet().stream()
                .map(vulnerability -> vulnerability.getItemSet().getItems())
                .filter( items -> items.size() != 1)
                .flatMap(Collection::stream)
                .map(Object::toString)
                .distinct()
                .sorted()
                .collect(Collectors.toList());
    }

    private List<String> getDirectIdentifiers(Map<IPVVulnerability, List<Integer>> vulnerabilities) {
        return vulnerabilities.keySet().stream()
                .map(vulnerability -> vulnerability.getItemSet().getItems())
                .filter( items -> items.size() == 1)
                .flatMap(Collection::stream)
                .map(Object::toString)
                .distinct()
                .sorted()
                .collect(Collectors.toList());
    }

    @Override
    public Dataset<Row> process(Dataset<Row> dataset) {

        int maxSize;
        Set<Integer> excludedFields;
        int k = taskOptions.getK();
        BruteWithDataset.findVulnerabilities(dataset, excludedFields, k, maxSize);


        private void writerReport(OutputStream output, Map<IPVVulnerability, List<Integer>> vulnerabilities) throws IOException {
            Map<String, Object> report = new HashMap<>();
            report.put("msg", "List of fields (zero-based) that violate the required k-uniqueness value");
            report.put("k", getTaskOptions().getK());
            report.put("direct-identifiers", getDirectIdentifiers(vulnerabilities));
            report.put("quasi-identifiers", getQuasiIdentifiers(vulnerabilities));

            mapper.writer().writeValue(output, report);
        }

        dataset.sparkSession().createDataFrame(
                vulne
                identifiedTypes.entrySet().stream().map(entry -> RowFactory.create(
                        entry.getKey(),
                        entry.getValue().getTypeName(),
                        fieldsProcessed.get(entry.getKey()).stream().collect(Collectors.toMap(
                                IdentifiedType::getTypeName,
                                IdentifiedType::getCount
                        )))
                ).collect(Collectors.toList()),
                new StructType(new StructField[]{
                        new StructField("K", DataTypes.StringType, false, Metadata.empty()),
                        new StructField("Type", DataTypes.StringType, false, Metadata.empty()),
                        new StructField("Field Names", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())
                })
        );
    }
}

