/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2019                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.vulnerability;

import com.ibm.research.drl.dpt.generators.ItemSet;
import com.ibm.research.drl.dpt.generators.LayerGenerator;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import scala.Tuple2;

import java.util.*;
import java.util.stream.Collectors;

import static org.apache.spark.sql.functions.*;

public class VulnerabilityDetectionWithBruteForceWithoutPruning extends VulnerabilityDetection {
    private static final Logger log = LoggerFactory.getLogger(VulnerabilityDetectionWithBruteForceWithoutPruning.class);

    private static final String AGGREGATED_COUNT_NAME = "____FICTIONAL___COLUMN___NAME___";
    private final Collection<Tuple2<List<String>, Long>> offendingCombinations = new ArrayList<>();

    @Override
    public Collection<List<String>> findVulnerabilities(Dataset<Row> original, int k, int maxSize) {
        final String[] columnNames = original.columns();
        final LayerGenerator generator = new LayerGenerator(columnNames.length);

        final Dataset<Row> dataset = original.cache();

        while (generator.hasNext()) {
            List<String> itemSet = generateItemset(columnNames, generator.next());

            log.debug("Testing {}", itemSet);

            long offendingRecords = countOffendingRecords(dataset, itemSet, k);

            if (offendingRecords > 0) {
                this.offendingCombinations.add(new Tuple2<>(itemSet, offendingRecords));
            }
        }

        return this.offendingCombinations.parallelStream().
                map(tuple -> tuple._1)
                .collect(Collectors.toList());
    }

    private long countOffendingRecords(Dataset<Row> dataset, List<String> itemSet, int k) {
        Column[] selectionColumn = convertToSelectionColumns(itemSet);

        Row[] rows = (Row[]) dataset.groupBy(selectionColumn)
                .agg(count(lit(1L)).as(AGGREGATED_COUNT_NAME))
                .filter(col(AGGREGATED_COUNT_NAME).lt(k))
                .agg(sum(col(AGGREGATED_COUNT_NAME)).as(AGGREGATED_COUNT_NAME))
                .collect();

        Row actualRow = rows[0];

        int idx = actualRow.fieldIndex(AGGREGATED_COUNT_NAME);

        if (!actualRow.isNullAt(idx)) {
            return actualRow.getLong(idx);
        }

        return 0;
    }

    private Column[] convertToSelectionColumns(List<String> itemSet) {
        Column[] columnNames = new Column[itemSet.size()];

        for (int i = 0; i < columnNames.length; ++i) {
            columnNames[i] = col(itemSet.get(i));
        }

        return columnNames;
    }

    private List<String> generateItemset(String[] columnNames, ItemSet itemIDs) {
        List<String> itemNames = new ArrayList<>(itemIDs.size());

        for (int id : itemIDs.getItems()) {
            itemNames.add(columnNames[id]);
        }

        return itemNames;
    }

    @Override
    protected Map<List<String>, Long> computeOffendingRecords(final Dataset<Row> dataset, final Collection<List<String>> vulnerabilities, final int k) {
        return this.offendingCombinations.parallelStream()
                .collect(Collectors.toMap(tuple2 -> tuple2._1, tuple2 -> tuple2._2));
    }
}
