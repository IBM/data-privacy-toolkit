/*
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
*/
package com.ibm.research.drl.dpt.spark.vulnerability;


import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.ibm.research.drl.dpt.configuration.DataTypeFormat;
import com.ibm.research.drl.dpt.datasets.DatasetOptions;
import com.ibm.research.drl.dpt.generators.IPVGenerator;
import com.ibm.research.drl.dpt.generators.ItemSet;
import com.ibm.research.drl.dpt.generators.LayerGenerator;
import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import org.apache.commons.cli.*;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.function.BiPredicate;
import java.util.stream.Collectors;

import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.count;
import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.functions.sum;

public class DUCCSpark {
    private static final Logger logger = LogManager.getLogger(DUCCSpark.class);
    private static final String AGGREGATED_COUNT_NAME = "____AGGREGATED____COUNT____NAME____";

    public static void main(String[] args) {
        Options options = new Options();

        Option confOption = new Option("c", "conf", true, "configuration file (required)");
        confOption.setRequired(true);

        options.addOption(confOption);

        Option i = new Option("i", "input", true, "input (required)");
        i.setRequired(true);
        options.addOption(i);
        Option o = new Option("o", "output", true, "output folder (required)");
        o.setRequired(true);
        options.addOption(o);

        options.addOption("remoteConf", false, "remote configuration files (optional)");

        options.addOption(new Option("report", false, "Report offending rows"));
        options.addOption("basePath", true, "Base path for reading partitioned data");
        options.addOption("remoteOutput", false, "save output remotely");

        try {
            CommandLine cmd = new DefaultParser().parse(options, args);

            boolean remoteConfiguration = cmd.hasOption("remoteConf");
            final VulnerabilityDetectionConfiguration configuration = SparkUtils.deserializeConfiguration(cmd.getOptionValue("c"), remoteConfiguration, VulnerabilityDetectionConfiguration.class);
            final Set<String> excludedFields = configuration.getExcludedFields();
            final int k = configuration.getK();

            JsonNode confFile = SparkUtils.readConfigurationFile(cmd.getOptionValue("c"), remoteConfiguration);

            logger.info("input: " + cmd.getOptionValue("i"));
            logger.info("output: " + cmd.getOptionValue("o"));
            logger.info("remote output save: " + cmd.hasOption("remoteOutput"));
            logger.info("configuration file: " + confFile.toString());

            final DataTypeFormat format = configuration.getDataTypeFormat();
            final DatasetOptions datasetOptions = configuration.getDatasetOptions();

            SparkContext sc = SparkUtils.createSparkContext("DUCCSpark");
            SparkSession spark = SparkSession.builder().sparkContext(sc).getOrCreate();

            DUCCSpark duccSpark = new DUCCSpark();

            String basePath = cmd.hasOption("basePath") ? cmd.getOptionValue("basePath") : null;
            logger.info("basePath " + basePath);

            final Dataset<Row> dataset = SparkUtils.createDataset(spark, cmd.getOptionValue("i"), format, datasetOptions, basePath);

            long totalRecords;

            final Collection<List<String>> vulnerabilities = duccSpark.findVulnerabilities(dataset, excludedFields, k);

            final Map<List<String>, Long> offendingRecords;
            if (cmd.hasOption("report")) {
                totalRecords = dataset.count();
                offendingRecords = duccSpark.computeOffendingRecords(dataset, vulnerabilities, k);
            } else {
                totalRecords = -1L;
                offendingRecords = null;
            }

            final VulnerabilityReport report = new VulnerabilityReport(configuration, vulnerabilities, offendingRecords, totalRecords);

            try (OutputStream outputStream = createOutputStream(cmd.getOptionValue("o"), cmd.hasOption("remoteOutput"))) {
                new ObjectMapper().writeValue(
                        outputStream,
                        report
                );
            }

            spark.stop();
        } catch (ParseException e) {
            logger.error(e.getMessage());

            HelpFormatter helpFormatter = new HelpFormatter();
            String header = "Vulnerability detection\n\n";
            String footer = "\n";
            helpFormatter.printHelp("VulnerabilityDetection", header, options, footer, true);
            System.exit(-1);
        } catch (IOException e) {
            e.printStackTrace();
            throw new RuntimeException(e);
        }
    }

    private static OutputStream createOutputStream(String outputPath, boolean remoteOutput) throws IOException {
        if (remoteOutput) {
            return SparkUtils.createHDFSOutputStream(outputPath);
        } else {
            return new FileOutputStream(outputPath);
        }
    }

    protected Map<List<String>, Long> computeOffendingRecords(final Dataset<Row> dataset, final Collection<List<String>> vulnerabilities, final int k) {
        Map<List<String>, Long> result = new HashMap<>(vulnerabilities.size());

        for (final List<String> vulnerability : vulnerabilities) {
            result.put(vulnerability, computeOffendingRecords(dataset, vulnerability, lit(k)));
        }

        return result;
    }

    protected long computeOffendingRecords(Dataset<Row> dataset, List<String> vulnerability, Column k) {
        Row[] rows = (Row[]) dataset.groupBy(buildGroupBy(vulnerability))
                .agg(count(lit(1)).as(AGGREGATED_COUNT_NAME))
                .filter(col(AGGREGATED_COUNT_NAME).lt(k))
                .agg(sum(col(AGGREGATED_COUNT_NAME)).as(AGGREGATED_COUNT_NAME))
                .collect();

        Row actualRow = rows[0];
        int idx = actualRow.fieldIndex(AGGREGATED_COUNT_NAME);

        return actualRow.getLong(idx);
    }

    private Column[] buildGroupBy(List<String> vulnerability) {
        final Column[] groupBy = new Column[vulnerability.size()];

        for (int i = 0; i < groupBy.length; ++i) {
            groupBy[i] = col(vulnerability.get(i));
        }

        return groupBy;
    }
    
    public Collection<List<String>> findVulnerabilities(final Dataset<Row> dataset, final Set<String> excludedFieldNames, final int k) {
        int numberOfColumns = extractNumberOfColumns(dataset);

        List<String> columnNames = Arrays.asList(dataset.columns());
        Set<Integer> excludedFields = toIndex(columnNames, excludedFieldNames);
        final List<Integer> uniques = uniqueColumns(dataset, k, excludedFields);
        final List<Set<Integer>> knownNotSafe = new ArrayList<>();
        final List<Set<Integer>> knownSafe = new ArrayList<>();

        for (Integer uniq : uniques) {
            knownNotSafe.add(Collections.singleton(uniq));
        }

        final List<Integer> validSeeds = findValidSeeds(numberOfColumns, uniques, excludedFields);

        final Collection<Integer> attributesToCheck = new ArrayList<>(validSeeds);
        for (final Integer validSeed : validSeeds) {
            attributesToCheck.remove((Object) validSeed);
            explorationPhase(dataset, Collections.singleton(validSeed), new ArrayList<>(attributesToCheck), 0, k, knownSafe, knownNotSafe);
        }

        return generatorFromNotSafe(knownNotSafe, numberOfColumns).getBanned().parallelStream().
                map(itemSet -> {
                    List<String> vulnerableFields = new ArrayList<>(itemSet.size());

                    for (int item : itemSet.getItems()) {
                        vulnerableFields.add(columnNames.get(item));
                    }

                    return vulnerableFields;
                }).collect(Collectors.toList());
    }

    private Set<Integer> toIndex(List<String> columns, Set<String> excludedFieldNames) {
        final Set<Integer> indexes = new HashSet<>(excludedFieldNames.size());

        excludedFieldNames.forEach( name -> {
            int index = columns.indexOf(name);
            if (-1 != index) {
                indexes.add(index);
            }
        });

        return indexes;
    }

    private IPVGenerator generatorFromNotSafe(List<Set<Integer>> knownNotSafe, int numberOfColumns) {
        IPVGenerator generator = new LayerGenerator(numberOfColumns);
        for (Collection<Integer> known : knownNotSafe) {
            generator.ban(generateItemSet(known));
        }

        return generator;
    }

    private Column[] toGroupByCondition(Set<Integer> newItemset, String[] columns) {
        Column[] items = new Column[newItemset.size()];

        int i = 0;
        for (int item : newItemset) {
            items[i++] = col(columns[item]);
        }

        return items;
    }

    private ItemSet generateItemSet(final Collection<Integer> items) {
        final ItemSet itemSet = new ItemSet();

        for (Integer item : items) {
            itemSet.addItem(item);
        }

        return itemSet;
    }

    private boolean isNotUnique(Dataset<Row> dataset, final int kValue, Column... groupByCondition) {
        return 0L == dataset.groupBy(groupByCondition).
                agg(count(lit(1L)).as(AGGREGATED_COUNT_NAME)).
                filter(col(AGGREGATED_COUNT_NAME).lt(lit(kValue))).count();
    }

    private void explorationPhase(Dataset<Row> dataset, Set<Integer> baseItemset, Collection<Integer> availableAttributes, int currentLevel, int kValue, List<Set<Integer>> knownSafe, List<Set<Integer>> knownNotSafe) {
        final Collection<Integer> remainingAttributes = new ArrayList<>(availableAttributes);
        for (final Integer attribute : availableAttributes) {
            final Set<Integer> itemSet = new HashSet<>(baseItemset);
            itemSet.add(attribute);
            remainingAttributes.remove((Object) attribute);

            if (isKnownNotSafe(knownNotSafe, itemSet)) {
//                log.debug("{} is known not safe", itemSet);
                continue;
            }
            if (isKnownSafe(knownSafe, itemSet)) {
//                log.debug("{} is knowwn safe", itemSet);
                continue;
            }

            if (isNotUnique(dataset, kValue, toGroupByCondition(itemSet, dataset.columns()))) {
                addWithMinimality(knownSafe, itemSet, this::testInLeft);
                explorationPhase(dataset, itemSet, new ArrayList<>(remainingAttributes), currentLevel + 1, kValue, knownSafe, knownNotSafe);
            } else {
                addWithMinimality(knownNotSafe, itemSet, this::testInRight);
            }
        }
    }

    private boolean testInRight(Set<Integer> left, Set<Integer> right) {
        return right.containsAll(left);
    }

    private boolean testInLeft(Set<Integer> left, Set<Integer> right) {
        return left.containsAll(right);
    }

    private void addWithMinimality(List<Set<Integer>> knownItemSets, Set<Integer> itemSet, BiPredicate<Set<Integer>, Set<Integer>> test) {
        knownItemSets.removeIf(current -> test.test(current, itemSet));
        knownItemSets.add(itemSet);
    }

    private boolean isKnownNotSafe(List<Set<Integer>> knownNotSafe, Set<Integer> itemSet) {
        for (Set<Integer> known : knownNotSafe) {
            if (testInRight(known, itemSet)) return true;
        }
        return false;
    }

    private boolean isKnownSafe(List<Set<Integer>> knownSafe, Set<Integer> itemSet) {
        for (Set<Integer> known : knownSafe) {
            if (testInLeft(known, itemSet)) return true;
        }
        return false;
    }

    private int extractNumberOfColumns(Dataset<Row> dataset) {
        return dataset.columns().length;
    }

    private List<Integer> findValidSeeds(int numberOfColumns, List<Integer> uniques, Set<Integer> excludedFields) {
        List<Integer> allColumns = new ArrayList<>(numberOfColumns - uniques.size() - excludedFields.size());

        for (int columnId = 0; columnId < numberOfColumns; ++columnId) {
            if (!uniques.contains(columnId) && !excludedFields.contains(columnId)) {
                allColumns.add(columnId);
            }
        }

        return allColumns;
    }

    private List<Integer> uniqueColumns(Dataset<Row> dataset, final int k, Set<Integer> excludedFields) {
        String[] columnNames = dataset.columns();

        List<Integer> columns = new ArrayList<>();

        for (int columnId = 0; columnId < columnNames.length; ++columnId) {
            if (excludedFields.contains(columnId)) continue;
            String columnName = columnNames[columnId];

            Dataset<Row> selection = dataset.
                    groupBy(columnName).
                    agg(count(lit(1)).as(AGGREGATED_COUNT_NAME)).
                    filter(col(AGGREGATED_COUNT_NAME).lt(lit(k)));

            if (selection.count() != 0) {
                columns.add(columnId);
            }
        }

        return columns;
    }

}
