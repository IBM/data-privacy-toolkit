/*******************************************************************
 *                                                                 *
 * Copyright IBM Corp. 2016                                        *
 *                                                                 *
 *******************************************************************/
package com.ibm.research.drl.dpt.spark.vulnerability;


import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.ibm.research.drl.dpt.configuration.DataTypeFormat;
import com.ibm.research.drl.dpt.datasets.DatasetOptions;
import com.ibm.research.drl.dpt.generators.IPVGenerator;
import com.ibm.research.drl.dpt.generators.ItemSet;
import com.ibm.research.drl.dpt.generators.LayerGenerator;
import com.ibm.research.drl.dpt.spark.utils.SparkUtils;
import org.apache.commons.cli.*;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.function.BiPredicate;
import java.util.stream.Collectors;

import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.count;
import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.functions.sum;

public class DUCCSpark {
    private static final Logger logger = LogManager.getLogger(DUCCSpark.class);
    private static final String AGGREGATED_COUNT_NAME = "____AGGREGATED____COUNT____NAME____";

    public static void main(String[] args) {
        Options options = new Options();

        Option confOption = new Option("c", "conf", true, "configuration file (required)");
        confOption.setRequired(true);

        options.addOption(confOption);

        Option i = new Option("i", "input", true, "input (required)");
        i.setRequired(true);
        options.addOption(i);
        Option o = new Option("o", "output", true, "output folder (required)");
        o.setRequired(true);
        options.addOption(o);

        options.addOption("remoteConf", false, "remote configuration files (optional)");

        options.addOption(new Option("report", false, "Report offending rows"));
        options.addOption("basePath", true, "Base path for reading partitioned data");
        options.addOption("remoteOutput", false, "save output remotely");

        try {
            CommandLine cmd = new DefaultParser().parse(options, args);

            boolean remoteConfiguration = cmd.hasOption("remoteConf");
            final VulnerabilityDetectionConfiguration configuration = SparkUtils.deserializeConfiguration(cmd.getOptionValue("c"), remoteConfiguration, VulnerabilityDetectionConfiguration.class);
            final Set<String> excludedFields = configuration.getExcludedFields();
            final int k = configuration.getK();

            JsonNode confFile = SparkUtils.readConfigurationFile(cmd.getOptionValue("c"), remoteConfiguration);

            logger.info("input: " + cmd.getOptionValue("i"));
            logger.info("output: " + cmd.getOptionValue("o"));
            logger.info("remote output save: " + cmd.hasOption("remoteOutput"));
            logger.info("configuration file: " + confFile.toString());

            final DataTypeFormat format = configuration.getDataTypeFormat();
            final DatasetOptions datasetOptions = configuration.getDatasetOptions();

            SparkContext sc = SparkUtils.createSparkContext("DUCCSpark");
            SparkSession spark = SparkSession.builder().sparkContext(sc).getOrCreate();

            DUCCSpark duccSpark = new DUCCSpark();

            String basePath = cmd.hasOption("basePath") ? cmd.getOptionValue("basePath") : null;
            logger.info("basePath " + basePath);

            final Dataset<Row> dataset = SparkUtils.createDataset(spark, cmd.getOptionValue("i"), format, datasetOptions, basePath);

            long totalRecords;

            final Collection<List<String>> vulnerabilities = duccSpark.findVulnerabilities(dataset, excludedFields, k);

            final Map<List<String>, Long> offendingRecords;
            if (cmd.hasOption("report")) {
                totalRecords = dataset.count();
                offendingRecords = duccSpark.computeOffendingRecords(dataset, vulnerabilities, k);
            } else {
                totalRecords = -1L;
                offendingRecords = null;
            }

            final VulnerabilityReport report = new VulnerabilityReport(configuration, vulnerabilities, offendingRecords, totalRecords);

            try (OutputStream outputStream = createOutputStream(cmd.getOptionValue("o"), cmd.hasOption("remoteOutput"))) {
                new ObjectMapper().writeValue(
                        outputStream,
                        report
                );
            }

            spark.stop();
        } catch (ParseException e) {
            logger.error(e.getMessage());

            HelpFormatter helpFormatter = new HelpFormatter();
            String header = "Vulnerability detection\n\n";
            String footer = "\n";
            helpFormatter.printHelp("VulnerabilityDetection", header, options, footer, true);
            System.exit(-1);
        } catch (IOException e) {
            e.printStackTrace();
            throw new RuntimeException(e);
        }
    }

    private static OutputStream createOutputStream(String outputPath, boolean remoteOutput) throws IOException {
        if (remoteOutput) {
            return SparkUtils.createHDFSOutputStream(outputPath);
        } else {
            return new FileOutputStream(outputPath);
        }
    }

    protected Map<List<String>, Long> computeOffendingRecords(final Dataset<Row> dataset, final Collection<List<String>> vulnerabilities, final int k) {
        Map<List<String>, Long> result = new HashMap<>(vulnerabilities.size());

        for (final List<String> vulnerability : vulnerabilities) {
            result.put(vulnerability, computeOffendingRecords(dataset, vulnerability, lit(k)));
        }

        return result;
    }

    protected long computeOffendingRecords(Dataset<Row> dataset, List<String> vulnerability, Column k) {
        Row[] rows = (Row[]) dataset.groupBy(buildGroupBy(vulnerability))
                .agg(count(lit(1)).as(AGGREGATED_COUNT_NAME))
                .filter(col(AGGREGATED_COUNT_NAME).lt(k))
                .agg(sum(col(AGGREGATED_COUNT_NAME)).as(AGGREGATED_COUNT_NAME))
                .collect();

        Row actualRow = rows[0];
        int idx = actualRow.fieldIndex(AGGREGATED_COUNT_NAME);

        return actualRow.getLong(idx);
    }

    private Column[] buildGroupBy(List<String> vulnerability) {
        final Column[] groupBy = new Column[vulnerability.size()];

        for (int i = 0; i < groupBy.length; ++i) {
            groupBy[i] = col(vulnerability.get(i));
        }

        return groupBy;
    }
    
    public Collection<List<String>> findVulnerabilities(final Dataset<Row> dataset, final Set<String> excludedFieldNames, final int k) {
        int numberOfColumns = extractNumberOfColumns(dataset);

        List<String> columnNames = Arrays.asList(dataset.columns());
        Set<Integer> excludedFields = toIndex(columnNames, excludedFieldNames);
        final List<Integer> uniques = uniqueColumns(dataset, k, excludedFields);
        final List<Set<Integer>> knownNotSafe = new ArrayList<>();
        final List<Set<Integer>> knownSafe = new ArrayList<>();

        for (Integer uniq : uniques) {
            knownNotSafe.add(Collections.singleton(uniq));
        }

        final List<Integer> validSeeds = findValidSeeds(numberOfColumns, uniques, excludedFields);

        final Collection<Integer> attributesToCheck = new ArrayList<>(validSeeds);
        for (final Integer validSeed : validSeeds) {
            attributesToCheck.remove((Object) validSeed);
            explorationPhase(dataset, Collections.singleton(validSeed), new ArrayList<>(attributesToCheck), 0, k, knownSafe, knownNotSafe);
        }

        return generatorFromNotSafe(knownNotSafe, numberOfColumns).getBanned().parallelStream().
                map(itemSet -> {
                    List<String> vulnerableFields = new ArrayList<>(itemSet.size());

                    for (int item : itemSet.getItems()) {
                        vulnerableFields.add(columnNames.get(item));
                    }

                    return vulnerableFields;
                }).collect(Collectors.toList());
    }

    private Set<Integer> toIndex(List<String> columns, Set<String> excludedFieldNames) {
        final Set<Integer> indexes = new HashSet<>(excludedFieldNames.size());

        excludedFieldNames.forEach( name -> {
            int index = columns.indexOf(name);
            if (-1 != index) {
                indexes.add(index);
            }
        });

        return indexes;
    }

    private IPVGenerator generatorFromNotSafe(List<Set<Integer>> knownNotSafe, int numberOfColumns) {
        IPVGenerator generator = new LayerGenerator(numberOfColumns);
        for (Collection<Integer> known : knownNotSafe) {
            generator.ban(generateItemSet(known));
        }

        return generator;
    }

    private Column[] toGroupByCondition(Set<Integer> newItemset, String[] columns) {
        Column[] items = new Column[newItemset.size()];

        int i = 0;
        for (int item : newItemset) {
            items[i++] = col(columns[item]);
        }

        return items;
    }

    private ItemSet generateItemSet(final Collection<Integer> items) {
        final ItemSet itemSet = new ItemSet();

        for (Integer item : items) {
            itemSet.addItem(item);
        }

        return itemSet;
    }

    private boolean isNotUnique(Dataset<Row> dataset, final int kValue, Column... groupByCondition) {
        return 0L == dataset.groupBy(groupByCondition).
                agg(count(lit(1L)).as(AGGREGATED_COUNT_NAME)).
                filter(col(AGGREGATED_COUNT_NAME).lt(lit(kValue))).count();
    }

    private void explorationPhase(Dataset<Row> dataset, Set<Integer> baseItemset, Collection<Integer> availableAttributes, int currentLevel, int kValue, List<Set<Integer>> knownSafe, List<Set<Integer>> knownNotSafe) {
        final Collection<Integer> remainingAttributes = new ArrayList<>(availableAttributes);
        for (final Integer attribute : availableAttributes) {
            final Set<Integer> itemSet = new HashSet<>(baseItemset);
            itemSet.add(attribute);
            remainingAttributes.remove((Object) attribute);

            if (isKnownNotSafe(knownNotSafe, itemSet)) {
//                log.debug("{} is known not safe", itemSet);
                continue;
            }
            if (isKnownSafe(knownSafe, itemSet)) {
//                log.debug("{} is knowwn safe", itemSet);
                continue;
            }

            if (isNotUnique(dataset, kValue, toGroupByCondition(itemSet, dataset.columns()))) {
                addWithMinimality(knownSafe, itemSet, this::testInLeft);
                explorationPhase(dataset, itemSet, new ArrayList<>(remainingAttributes), currentLevel + 1, kValue, knownSafe, knownNotSafe);
            } else {
                addWithMinimality(knownNotSafe, itemSet, this::testInRight);
            }
        }
    }

    private boolean testInRight(Set<Integer> left, Set<Integer> right) {
        return right.containsAll(left);
    }

    private boolean testInLeft(Set<Integer> left, Set<Integer> right) {
        return left.containsAll(right);
    }

    private void addWithMinimality(List<Set<Integer>> knownItemSets, Set<Integer> itemSet, BiPredicate<Set<Integer>, Set<Integer>> test) {
        knownItemSets.removeIf(current -> test.test(current, itemSet));
        knownItemSets.add(itemSet);
    }

    private boolean isKnownNotSafe(List<Set<Integer>> knownNotSafe, Set<Integer> itemSet) {
        for (Set<Integer> known : knownNotSafe) {
            if (testInRight(known, itemSet)) return true;
        }
        return false;
    }

    private boolean isKnownSafe(List<Set<Integer>> knownSafe, Set<Integer> itemSet) {
        for (Set<Integer> known : knownSafe) {
            if (testInLeft(known, itemSet)) return true;
        }
        return false;
    }

    private int extractNumberOfColumns(Dataset<Row> dataset) {
        return dataset.columns().length;
    }

    private List<Integer> findValidSeeds(int numberOfColumns, List<Integer> uniques, Set<Integer> excludedFields) {
        List<Integer> allColumns = new ArrayList<>(numberOfColumns - uniques.size() - excludedFields.size());

        for (int columnId = 0; columnId < numberOfColumns; ++columnId) {
            if (!uniques.contains(columnId) && !excludedFields.contains(columnId)) {
                allColumns.add(columnId);
            }
        }

        return allColumns;
    }

    private List<Integer> uniqueColumns(Dataset<Row> dataset, final int k, Set<Integer> excludedFields) {
        String[] columnNames = dataset.columns();

        List<Integer> columns = new ArrayList<>();

        for (int columnId = 0; columnId < columnNames.length; ++columnId) {
            if (excludedFields.contains(columnId)) continue;
            String columnName = columnNames[columnId];

            Dataset<Row> selection = dataset.
                    groupBy(columnName).
                    agg(count(lit(1)).as(AGGREGATED_COUNT_NAME)).
                    filter(col(AGGREGATED_COUNT_NAME).lt(lit(k)));

            if (selection.count() != 0) {
                columns.add(columnId);
            }
        }

        return columns;
    }

}
