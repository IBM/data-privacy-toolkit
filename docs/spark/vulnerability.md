## Invoking the vulnerability detection Spark executor

The vulnerability executor requires the input and output paths as well as the configuration file.
It can be invoked as:

```
spark-submit \
	--class com.ibm.research.drl.dpt.spark.vulnerability.DUCCSpark \
	dpt-spark-$VERSION-jar-with-dependencies.jar \
	-c vulnerability_detection.json \
	-i test.csv \
	-o report.json
```

where `vulnerability_detection.json` is the configuration file, `test.csv` is the input file and `report.json` is the output filename to save the report.

The full list of parameters to the executor is:

| Parameter          | Description                                                           |
|--------------------|-----------------------------------------------------------------------|
| -i,--input <arg>   | Path to the file to be masked (required)                              |
| -o,--output <arg>  | Output file (required)                                                |
| -c,--conf <arg>    | Path to the configuration file (required)                             |
| --remoteConf       | If set, then the configuration file will be read from HDFS (optional) |
| --remoteOutput     | If set, the report will be saved to HDFS                              |
| --basePath \<arg\> | Specify the base path of the input (optional)                         |
| --report           | Report offending records (optional)                                   |

The `--remoteConf` option controls if the configuration file should be read from HDFS instead of the local filesystem.

## Writing the configuration file

The vulnerability detection action requires a configuration file in order to understand the input format and to control the identification behavior.

This is a sample template for the masking configuration:

```json
{
  "k": 5,
  "inputFormat": "CSV",
  "excludedFields": [
    "c0"
],
  "datasetOptions": {
    "delimiter": ",",
    "quoteChar": "\"",
    "hasHeader": true,
    "trimFields": true
  }
}
```

**Explanation**

* *inputFormat*: defines the input format for the export.

The following data input formats are supported:

| Data input format type enumeration |
|------------------------------------|
| CSV                                |
| JSON                               |
| DICOM                              |
| XLS                                |
| XLSX                               |
| XML                                |
| HL7                                |
| FHIR_JSON                          |
| PARQUET                            |

* *k* specifies the minimum size required for each equivalence class identified per attribute combination.
* *excludedFields* defines an array of field names (like `Column 0`, `_c0`, `USERID`) that needs to be excluded in the exploration of the dataset.

* *datasetOptions* contains a input-format specific options. As an example, the inputFormat CSV supports the following options:
    * `quoteChar`,
    * `delimiter`,
    * `hasHeader`, and
    * `trimFields`.

**Example for CSV files**

In this example, we will inspect a CSV file, requing that each equivalence class has at least 5 members and ecluding column `c0` and we will only consider the first 1000 rows

```json
{
  "k": 5,
  "inputFormat": "CSV",
  "excludedFields": [
    "c0"
  ],
  "datasetOptions": {
    "delimiter": ",",
    "quoteChar": "\"",
    "hasHeader": false,
    "trimFields": true
  }
}
```



## Output structure

The output will be a dictionary having three fields.

* *k*, the k-value specified in the configuration,
* *excludedFields*, the list of fields marked to be excluded from the analysis, as spefified in the configuration
* *vulnerabilities*, the list of identified vulnerabilities. This field is a list  of lists of strings, each of which specifies the list of colum names (either retrieved from the configuration or autogenerated).
* *offendingRecords* will contain the number of offending records for each vulnerability if `--report` is specified in command line parameters else it will be empty
* *totalRecords* contains the total number of records in the analysed dataset

 
The keys to each section will be the field names. For each key the value is an array of the identified types information, which includes the data type as `typeName` and the number of instances that was found in the data as `count`.

```json
{
  "k": 5,
  "excludedFields": [],
  "vulnerabilities": [
    [ "Column 0" ],
    [ "Column 2" ],
    [ "Column 1", "Column 4" ],
    [ "Column 1", "Column 5" ],
    [ "Column 1", "Column 6" ]
  ],
  "offendingRecords": {
   	"[Column 0]": 1000,
   	"[Column 2]": 1200,
   	"[Column 1,Column 4]": 1000,
   	"[Column 1,Column 5]": 1000,
   	"[Column 1,Column 6]": 1000
  },
  "totalRecords": 10000
}
```

## Errors

A runtime exception will be thrown at the following cases:

* misconfiguration in the configuration file (wrong types or values)
* the input file cannot be found
* the user has no permission to read the input file or write to the output directory



